\chapter{Fondamenti Teorici}
\label{chap:chap2}

\section{Parte I: Logica Fuzzy e Clustering}

\subsection{Insiemi Fuzzy}

Un insieme fuzzy~\cite{Goguen_1973} è una generalizzazione dell'insieme classico in cui la nozione di appartenenza è graduata e rappresentata da una funzione $\mu_A(x): X \rightarrow [0,1]$, denominata come \textit{funzione di appartenenza}. Il valore restituito indica il grado con cui un elemento $x$ appartiene all'insieme fuzzy $A$. 

\subsubsection{Interpretazione di un insieme fuzzy}

L’idea centrale dei sistemi fuzzy non è solo usare valori tra 0 e 1, ma fornire un’interpretazione semantica adeguata alla funzione di appartenenza. Secondo~\cite{DUBOIS1997141}, tali interpretazioni possono rappresentare similarità, preferenze o incertezza:
\begin{itemize}
    \item \textbf{Similarità}: L'utilizzo del grado di appartenenza come similarità sottintende l'idea che ci siano elementi tipici di un determinato insieme, altri che sicuramente non vi appartengono e una gradualità di sfumature per i rimanenti.
    \item \textbf{Incertezza}: La funzione di appartenenza viene utilizzata per dotare di un grado l'incertezza relativa ad un qualche fatto.
    \item \textbf{Preferenza}: Il grado di appartenenza viene utilizzato per esprimere preferenze graduali rispetto a determinati criteri. Questa interpretazione trova il suo principale utilizzo in teoria delle decisioni e ha favorito lo sviluppo di un'ampia letteratura, in particolare sulle modalità di aggregare diversi insiemi fuzzy.
\end{itemize}

\subsubsection{Estensione e operazioni fuzzy}

Le operazioni insiemistiche di unione, intersezione e complemento vengono estese agli insiemi fuzzy con il metodo pointwise, ovvero fissando un elemento ed utilizzando solo il suo grado di appartenenza agli insiemi fuzzy coinvolti:
\begin{itemize}
  \item \textbf{Intersezione}: $\mu_{A \cap B}(x) = \min(\mu_A(x), \mu_B(x))$
  \item \textbf{Unione}: $\mu_{A \cup B}(x) = \max(\mu_A(x), \mu_B(x))$
  \item \textbf{Complemento}: $\mu_{\neg A}(x) = 1 - \mu_A(x)$
\end{itemize}
Tali operatori possono essere generalizzati tramite t-norme e t-conorme, per ottenere maggiore flessibilità computazionale.

\subsubsection{Variabili Linguistiche}

L’approccio linguistico fuzzy~\cite{890332} è utile per modellare preferenze incerte o vaghe, utilizzando variabili linguistiche~\cite{ZADEH1975199}, i cui termini sono rappresentati tramite funzioni di appartenenza fuzzy.

\subsubsection{Granular Computing}
Una disciplina emersa grazie agli insiemi fuzzy è la granular computing~\cite{doi:10.1142/9789814261302_0022}, cioè la disciplina che si occupa di rappresentare e processare l’informazione sotto forma di qualche tipo di aggregato, generalmente chiamato granulo di informazione, risultato di un processo di astrazione o estrazione di conoscenza dai dati.

\subsubsection{Fuzzy Clustering}

L'obiettivo del \textit{clustering} è raggruppare elementi simili tra loro o simili ad uno o 
più elementi centrali, in modo che i diversi gruppi siano il più possibile 
omogenei al loro interno e allo stesso tempo diversi tra loro. È quindi uno dei  metodi utilizzati nella granular computing per definire i granuli. 

Quel che si ottiene nella variante fuzzy prende il nome di \textit{Fuzzy Clustering}~\cite{dunn1973fuzzy}, dove i diversi gruppi non sono separati tra loro ma possono essere sovrapposti, e dunque un oggetto può appartenere a gruppi diversi, con un determinato grado di appartenenza. L’algoritmo più noto è il \textbf{Fuzzy C-Means (FCM)}~\cite{BEZDEK1984191}, che generalizza il classico K-Means minimizzando la funzione:

\[
J_m = \sum_{i=1}^{N} \sum_{j=1}^{C} u_{ij}^m \cdot \|x_i - c_j\|^2
\]

dove $u_{ij}$ è il grado di appartenenza dell’elemento $x_i$ al centro $c_j$, e $m > 1$ è il parametro di fuzzificazione. L’algoritmo aggiorna iterativamente i centroidi e le appartenenze per ottenere una partizione fuzzy dei dati.

Il parametro di fuzzificazione $m$ controlla la "morbidezza" della partizione: per $m \rightarrow 1$ il clustering tende a diventare più rigido, simile al K-Means classico, mentre per valori crescenti di $m$ l'appartenenza degli oggetti ai cluster diventa più uniforme. Valori tipici di $m$ sono compresi tra 1.5 e 2.5~\cite{BEZDEK1984191}.


\section{Parte II: Sistemi di Raccomandazione}

\subsection{Sistemi di raccomandazione}

I \textit{sistemi di raccomandazione} sono stati introdotti nel 1992 da Goldberg\cite{10.1145/138859.138867}. Secondo~\cite{burke2002hybrid}, un sistema di raccomandazione è definito come ``qualsiasi sistema che produce raccomandazioni personalizzate in output o ha l’effetto di guidare l’utente in modo personalizzato verso oggetti interessanti o utili in un ampio spazio di opzioni possibili''. 

\subsubsection{Classificazione}

Una delle classificazioni dei sistemi di raccomandazione più popolari è quella proposta da Bobadilla et al.~\cite{bobadilla2013recommender}, che distingue:

\begin{itemize}
    \item \textbf{Filtraggio demografico}: utilizza attributi dell’utente (es. età, sesso, localizzazione) per identificare preferenze simili tra utenti con caratteristiche comuni~\cite{bobadilla2013recommender, zhao2014we}.

    \item \textbf{Filtraggio collaborativo}: sfrutta unicamente le valutazioni espresse dagli utenti~\cite{adomavicius2005toward}. I suggerimenti sono generati confrontando le valutazioni di utenti simili.
    
    \item \textbf{Raccomandazione basata sul contenuto}: utilizza descrizioni degli oggetti e un profilo dell’utente per raccomandare elementi simili a quelli già apprezzati~\cite{ de2015semantics, lops2011content}.
    
    \item \textbf{Approcci ibridi}: combinano più paradigmi, come filtraggio collaborativo e demografico~\cite{vozalis2007using}, o collaborativo e basato sul contenuto~\cite{balabanovic1997fab}. Burke~\cite{burke2002hybrid} ha identificato sei tecniche principali di ibridazione.
\end{itemize}

\subsubsection{Filtraggio Collaborativo}

Il \emph{filtraggio collaborativo} (CF) è una delle tecniche più diffuse nei sistemi di raccomandazione e si basa sull’idea che utenti che hanno espresso giudizi simili in passato tenderanno a condividere gusti simili anche in futuro~\cite{adomavicius2005toward}. 

Gli algoritmi CF si dividono generalmente in due macro-categorie~\cite{LU20121}:

\begin{itemize}
    \item \textbf{Approcci Memory-Based}: utilizzano l’intero database di valutazioni utente-oggetto per generare raccomandazioni in tempo reale. Le raccomandazioni vengono prodotte direttamente confrontando la similarità tra utenti o tra oggetti, senza costruire un modello esplicito. Questi approcci si distinguono ulteriormente in:
    \begin{itemize}
        \item \textbf{User-Based}: confrontano le valutazioni di un utente target con quelle di altri utenti, individuando i “vicini” più simili. Le preferenze degli utenti simili vengono poi aggregate per prevedere le valutazioni dell’utente target.
        \item \textbf{Item-Based}: analizzano la similarità tra gli oggetti (es. film, prodotti) sulla base delle valutazioni espresse dagli utenti, ipotizzando che un utente apprezzerà oggetti simili a quelli già valutati positivamente.
    \end{itemize}
    \item \textbf{Approcci Model-Based}: costruiscono un modello predittivo a partire dai dati storici, spesso impiegando tecniche di machine learning come clustering, regressione o decomposizione matriciale. I modelli ottenuti operano su una rappresentazione ridotta dei dati, e sono in grado di affrontare problemi come la scalabilità e la sparsità della matrice delle valutazioni.
\end{itemize}

\subsubsection{Approccio User-Based} 

Nel CF \emph{user-based}, si parte da una matrice utente-oggetto $n \times m$ contenente le valutazioni espresse da $n$ utenti su $m$ oggetti. Quando un nuovo utente entra nel sistema, si identificano utenti “simili” (i \emph{vicini}) e si usano le loro valutazioni per predire le preferenze dell’utente target.

La similarità tra utenti viene solitamente calcolata con misure classiche come:

\begin{itemize}
    \item \textbf{Pearson correlation coefficient};
    \item \textbf{Cosine similarity};
    \item \textbf{Jaccard index}, ecc.
\end{itemize}

Il coefficiente di Pearson tra due utenti $a$ e $b$ è definito da:

\[
\text{sim}(a,b) = \frac{\sum_{p \in P}(r_{a,p} - \bar{r}_a)(r_{b,p} - \bar{r}_b)}{\sqrt{\sum_{p \in P}(r_{a,p} - \bar{r}_a)^2} \sqrt{\sum_{p \in P}(r_{b,p} - \bar{r}_b)^2}}
\]

dove $P$ è l’insieme degli item valutati da entrambi, e $r_{a,p}$ è la valutazione dell’utente $a$ sull’oggetto $p$.

Una volta individuati i vicini, la previsione della valutazione dell’utente target su un oggetto $p$ può essere calcolata con una media pesata:

\[
\text{pred}(a,p) = \bar{r}_a + \frac{\sum_{b \in N} \text{sim}(a,b) (r_{b,p} - \bar{r}_b)}{\sum_{b \in N} |\text{sim}(a,b)|}
\]

oppure con una semplice media delle valutazioni:

\[
\text{pred}(a,p) = \frac{1}{n} \sum_{i=1}^n r_{i,p}
\]

Infine, gli oggetti con le valutazioni predette più alte vengono raccomandati all’utente.

\section{Parte III: Clustering nei Sistemi di Raccomandazione}

\subsection{Clustering nei Sistemi di Raccomandazione}

Le misure di similarità, sebbene diffuse, presentano limiti rilevanti:
\begin{itemize}
    \item possono essere computazionalmente onerose su dataset ampi;
    \item risultano inefficaci in presenza di dati sparsi (\emph{sparsity problem});
    \item non catturano relazioni strutturali latenti tra utenti.
\end{itemize}

Per superare tali limiti, si ricorre spesso a tecniche di \textbf{clustering}, che organizzano gli utenti in gruppi omogenei. A ogni cluster possono essere associati contenuti preferiti, da raccomandare agli utenti che vi appartengono.

\subsubsection{Approcci di Clustering}

Tra le tecniche di clustering più utilizzate in ambito CF troviamo:

\begin{itemize}
    \item \textbf{K-Means}: produce partizioni disgiunte; semplice ed efficiente, ma assume cluster sferici e richiede $k$ fissato a priori;
    \item \textbf{SOM (Self-Organizing Maps)}: rete neurale non supervisionata che proietta i dati in uno spazio ridotto conservando relazioni topologiche;
    \item \textbf{Fuzzy Clustering (es. FCM)}: consente appartenenze multiple; utile per modellare utenti con gusti sfaccettati.
\end{itemize}

L’uso del clustering nei sistemi di raccomandazione consente di:
\begin{itemize}
    \item ridurre la dimensionalità del problema (scalabilità);
    \item mitigare il problema della sparsità;
    \item selezionare i vicini tra gli utenti dello stesso cluster, migliorando la coerenza del filtraggio.
\end{itemize}
